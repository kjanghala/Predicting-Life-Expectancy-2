{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Model to Predict Life Expectancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Model Trained and Tested Randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data file\n",
    "data = pd.read_csv('data/derived/final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify independent and dependent variables \n",
    "X = data[['state', 'year', 'quartile', 'gender']]\n",
    "y = data['LE'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical variables to binary\n",
    "X = pd.get_dummies(X, columns=['state', 'gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kanika\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Kanika\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n",
      "C:\\Users\\Kanika\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DataConversionWarning: Data with input dtype uint8, int64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# scale data\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "y_scaler = StandardScaler().fit(y_train)\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "y_train_scaled = y_scaler.transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "# plt.scatter(model.predict(X_train_scaled), model.predict(X_train_scaled) - y_train_scaled, c=\"blue\", label=\"Training Data\")\n",
    "# plt.scatter(model.predict(X_test_scaled), model.predict(X_test_scaled) - y_test_scaled, c=\"orange\", label=\"Testing Data\")\n",
    "# plt.legend()\n",
    "# plt.hlines(y=0, xmin=y_test_scaled.min(), xmax=y_test_scaled.max())\n",
    "# plt.title(\"Residual Plot\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.2007051164457062, Training R2: 0.7992948835542938\n",
      "Testing MSE: 0.23173310859021998, Testing R2: 0.7707976555734117\n"
     ]
    }
   ],
   "source": [
    "train_predictions = model.predict(X_train_scaled)\n",
    "training_mse = mean_squared_error(y_train_scaled, train_predictions)\n",
    "train_r2 = model.score(X_train_scaled, y_train_scaled)\n",
    "\n",
    "test_predictions = model.predict(X_test_scaled)\n",
    "testing_mse = mean_squared_error(y_test_scaled, test_predictions)\n",
    "test_r2 = model.score(X_test_scaled, y_test_scaled)\n",
    "\n",
    "print(f\"Training MSE: {training_mse}, Training R2: {train_r2}\")\n",
    "print(f\"Testing MSE: {testing_mse}, Testing R2: {test_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Model and Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/model_scaler.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export model\n",
    "production = LinearRegression()\n",
    "production.fit(X_train_scaled, y_train)\n",
    "dump(production, 'model/model.joblib')\n",
    "dump(X_scaler, 'model/model_scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for state names\n",
    "us_state_abbrev = {\n",
    "    'AL': 'Alabama',\n",
    "    'AK': 'Alaska',\n",
    "    'AZ': 'Arizona',\n",
    "    'AR': 'Arkansas',\n",
    "    'CA': 'California',\n",
    "    'CO': 'Colorado',\n",
    "    'CT': 'Connecticut',\n",
    "    'DE': 'Delaware',\n",
    "    'DC': 'District of Columbia',\n",
    "    'FL': 'Florida',\n",
    "    'GA': 'Georgia',\n",
    "    'HI': 'Hawaii',\n",
    "    'ID': 'Idaho',\n",
    "    'IL': 'Illinois',\n",
    "    'IN': 'Indiana',\n",
    "    'IA': 'Iowa',\n",
    "    'KS': 'Kansas',\n",
    "    'KY': 'Kentucky',\n",
    "    'LA': 'Louisiana',\n",
    "    'ME': 'Maine',\n",
    "    'MD': 'Maryland',\n",
    "    'MA': 'Massachusetts',\n",
    "    'MI': 'Michigan',\n",
    "    'MN': 'Minnesota',\n",
    "    'MS': 'Mississippi',\n",
    "    'MO': 'Missouri',\n",
    "    'MT': 'Montana',\n",
    "    'NE': 'Nebraska',\n",
    "    'NV': 'Nevada',\n",
    "    'NH': 'New Hampshire',\n",
    "    'NJ': 'New Jersey',\n",
    "    'NM': 'New Mexico',\n",
    "    'NY': 'New York',\n",
    "    'NC': 'North Carolina',\n",
    "    'ND': 'North Dakota',\n",
    "    'OH': 'Ohio',\n",
    "    'OK': 'Oklahoma',\n",
    "    'OR': 'Oregon',\n",
    "    'PA': 'Pennsylvania',\n",
    "    'RI': 'Rhode Island',\n",
    "    'SC': 'South Carolina',\n",
    "    'SD': 'South Dakota',\n",
    "    'TN': 'Tennessee',\n",
    "    'TX': 'Texas',\n",
    "    'UT': 'Utah',\n",
    "    'VT': 'Vermont',\n",
    "    'VA': 'Virginia',\n",
    "    'WA': 'Washington',\n",
    "    'WV': 'West Virginia',\n",
    "    'WI': 'Wisconsin',\n",
    "    'WY': 'Wyoming'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data files\n",
    "data = pd.read_csv('data/derived/final_data.csv')\n",
    "income_quartile = pd.read_csv('data/derived/ref_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe that can be used to select state inputs for model \n",
    "sel_state = pd.DataFrame(data['state'])\n",
    "sel_state['state'] = sel_state['state'].str.upper()\n",
    "sel_state['st_name'] = sel_state['state'].map(us_state_abbrev)\n",
    "sel_state = pd.get_dummies(sel_state, columns=['state'])\n",
    "sel_state = sel_state.drop_duplicates()\n",
    "sel_state = sel_state.rename(columns={\"st_name\": \"state\"})\n",
    "sel_state['state'] = sel_state['state'].str.lower()\n",
    "sel_state = sel_state.set_index('state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe that can be used to select income inputs for model \n",
    "income_quartile = income_quartile.rename(columns={\"State\": \"state\", \"_25th_Percentile\": \"_25\", \"Median\": \"_50\", \"_75th_Percentile\": \"_75\"})\n",
    "income_quartile['state'] = income_quartile['state'].str.lower()\n",
    "income_quartile = income_quartile.set_index('state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data frames\n",
    "sel_state.to_csv(path_or_buf='data/derived/sel_state.csv')\n",
    "income_quartile.to_csv(path_or_buf='data/derived/income_quartile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that creates input for model based on values provided\n",
    "sel_state = pd.read_csv('data/derived/sel_state.csv')\n",
    "sel_state = sel_state.set_index('state')\n",
    "income_quartile = pd.read_csv('data/derived/income_quartile.csv')\n",
    "income_quartile = income_quartile.set_index('state')\n",
    "\n",
    "def user_input(u_year, u_income, u_state, u_gender):\n",
    "    user_df = pd.DataFrame()\n",
    "    user_df['year'] = [u_year]\n",
    "    u_state = u_state.lower()\n",
    "    if u_income <= income_quartile.loc[u_state, '_25']:\n",
    "        user_df['quartile'] = [1]\n",
    "    elif u_income <= income_quartile.loc[u_state, '_50']:\n",
    "        user_df['quartile'] = [2]\n",
    "    elif u_income <= income_quartile.loc[u_state, '_75']:\n",
    "        user_df['quartile'] = [3]\n",
    "    else:\n",
    "        user_df['quartile'] = [4]\n",
    "    user_df['state'] = u_state\n",
    "    new_df = sel_state.loc[[u_state]]\n",
    "    user_df = pd.merge(user_df, new_df, on = 'state')\n",
    "    user_df = user_df.drop(columns=['state'])  \n",
    "    u_gender = u_gender.lower()\n",
    "    if u_gender == 'female':\n",
    "        x = 1\n",
    "        y = 0\n",
    "    elif u_gender == 'male':  \n",
    "        x = 0\n",
    "        y = 1\n",
    "    user_df['gender_Female'] = [x]\n",
    "    user_df['gender_Male'] = [y] \n",
    "    return user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model and scaler\n",
    "model = load('model/model.joblib')\n",
    "X_scaler = load('model/model_scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[82.51622101]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kanika\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "mytest = user_input(2020, 10000, 'Alabama', 'Female')\n",
    "mytest_scaled = X_scaler.transform(mytest)\n",
    "predict_le = model.predict(mytest_scaled)\n",
    "print(predict_le)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
